import numpy
import math, random


class NeuraltNettverk():

    def __init__(self,nh,c,d):
        # Argumenter;  nh = gjemte noder, c= antall klasser, d= feature vektor dimensjon

        # Omraadet vektene skal ligge mellom og antall utganger.
        self.nh = nh
        self.initVekt = 1/math.sqrt(self.nh)
        self.antallKlasser = c
        self.antallInput = d

        # Learning raten til det neurale nettverket (mulighet for aa kunne endre)
        self.larerate = 1

        # Lager utgangsvektor fra den gjemte noden og utgangen
        self.y_Hidden = numpy.zeros((self.nh,1))
        self.z_Utgang = numpy.zeros((self.antallKlasser,1))

        # DUMMY VEKTOR. MAA ERSTATTES!
        self.target = numpy.array([1, 0])

        # Utgangsvektor fra det neurale nettverket
        self.z_Utgang = numpy.empty((self.antallKlasser,1))

        # Lager tomme matriser for vektene
        self.vektItH = numpy.empty((self.nh, self.antallInput))
        self.vektHtO = numpy.empty((self.antallKlasser,self.nh))

        # Lager tomme vektorer for ItH, HtO Bias og bias vekt
        self.ItHBias = numpy.ones((self.nh,1))
        self.vektBiasItH = numpy.empty((self.nh,1))
        self.HtOBias = numpy.ones((self.antallKlasser,1))
        self.vektBiasHtO = numpy.empty((self.antallKlasser,1))


    def lageVekter(self):
    # Denne metoden initerer vekter og bias til det neurale nettverket.

        # Fyller inn verdier i nodevekt og bias.
        for rad in range(0, self.nh):

            # Lager biasvekt for Input to Hidden
            self.vektBiasItH[rad,0] = 0.5*math.pow(-1,rad)

            # Lager biasvekt for Hidden to Output
            if rad <= self.antallKlasser-1:
                self.vektBiasHtO[rad,0] = 0.5*math.pow(-1,rad)

            # Lager Input to Hidden nodevekt i omraadet -1/sqrt(nh)<noder<1/sqrt(nh).
            for kol in range(0,self.antallInput):
                self.vektItH[rad, kol] = (random.randrange(-100,100) * self.initVekt)/100

                # Lager Hidden to Output nodevekt i omraadet -1/sqrt(nh)<noder<1/sqrt(nh).
                if kol <= self.antallKlasser-1:
                    self.vektHtO[kol, rad] = (random.randrange(-100,100) * self.initVekt)/100


    def FeedForward(self):
        # Regner ut FeedForward for det neurale nettverket.

        # Test variabler. FJERNES I ETTERKANT!
        self.inngangsVektor = numpy.ones((self.antallInput,1))

        # Finner "net_j" for Input to Hidden
        self.netHidden = (self.vektItH).dot(self.inngangsVektor) + self.ItHBias*self.vektBiasItH

        # Finner utgangen til den gjemte noden. Bruker aktiveringsfunkjson paa formen 1/(1+e^(net))
        for node in range(0,self.nh):
            self.y_Hidden[node,0] = 1/(1 + math.exp(self.netHidden[node,0]))

        # Finner "net" for Hidden to Output.
        self.netUtgang = (self.vektHtO).dot(self.y_Hidden) + self.HtOBias*self.vektBiasHtO

        # Regner ut utgangsverdiene z
        for node in range(0,self.antallKlasser):
            self.z_Utgang[node,0] = 1/(1 + math.exp(self.netUtgang[node,0]))

        # Returnerer utgangsvektor, der hver element beskrive en klasse.
        return self.z_Utgang


    def Backward(self):
        # DENNE METODEN MAA SJEKKES! MANGE FALLGRUVER

        # Finner f(net_1) og f(net_2) derivert
        y_utgangDerivert = self.y_Hidden.T*(numpy.ones((1,self.nh)) - self.y_Hidden.T)
        z_utgangDerivert = self.z_Utgang.T*(numpy.ones((1,self.antallKlasser)) - self.z_Utgang.T)

        # Finner sensitiviteten til Hidden to Output og Input to Hidden
        sensitivitetHtO = (((self.target - self.z_Utgang.T))*z_utgangDerivert).T
        sensitivitetItH = y_utgangDerivert.T*self.vektHtO.T.dot(sensitivitetHtO)

        # Finner delta vekt HtO og ItH
        deltaVektHtO = sensitivitetHtO.dot(self.y_Hidden.T)
        deltaVektItH = sensitivitetItH.dot(self.inngangsVektor.T)

        # Finner delta vekt bias HtO og ItH
        # SPORRE TRYGVE! Dersom Y0 er satt til 1, saa vil delta vekt bias = sensitiviteten?
        deltaVektBiasHtO = sensitivitetHtO
        deltaVektBiasItH = sensitivitetItH

        # Oppdaterer Input to Hidden vektene
        self.vektItH = self.vektItH + deltaVektItH
        self.vektBiasItH = self.vektBiasItH + deltaVektBiasItH

        # Oppdaterer Hidden to Output vektene
        self.vektHtO = self.HtOBias + deltaVektHtO
        self.vektBiasHtO = self.vektBiasHtO + deltaVektBiasHtO

        return self.vektItH
        # Repetere for de siste vektene.







Program = NeuraltNettverk(20,2,10)
Program.lageVekter()
matrise = Program.FeedForward()
test = Program.Backward()

print(test.shape)






